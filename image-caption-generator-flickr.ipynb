{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedmoez2/image-caption-generator-flickr?scriptVersionId=134609822\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# for managing files\nimport os\n# for storing features\nimport pickle\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n# for providing UI about how much data processed\nfrom tqdm.notebook import tqdm\n# for extracting features from images data, and preprocessing data for model\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n# for text preprocessing, such as converting text to lowercase, filtering out stop words\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n# for pading or truncating the sequence so that all sequences have the same length\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n# for compiling the model with loss function, optimizer or dataset training\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.models as tf_models\n# for converting labels or words into vectors (1 or 0), and model visualization\nfrom tensorflow.keras.utils import to_categorical, plot_model\n# for importing model layers\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:14.783188Z","iopub.execute_input":"2023-06-23T07:07:14.783549Z","iopub.status.idle":"2023-06-23T07:07:20.706333Z","shell.execute_reply.started":"2023-06-23T07:07:14.783505Z","shell.execute_reply":"2023-06-23T07:07:20.705307Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset_dir = '/kaggle/input/flickr8k'\nfeatures_dir = '/kaggle/input/Flickr-features'\nwork_dir = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:20.708224Z","iopub.execute_input":"2023-06-23T07:07:20.708808Z","iopub.status.idle":"2023-06-23T07:07:20.719291Z","shell.execute_reply.started":"2023-06-23T07:07:20.70877Z","shell.execute_reply":"2023-06-23T07:07:20.718262Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# load vgg16 model\nmodel = VGG16()\n# restructure the model\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n# summarize\n#print(model.summary())","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-06-23T07:07:20.720851Z","iopub.execute_input":"2023-06-23T07:07:20.721372Z","iopub.status.idle":"2023-06-23T07:07:48.296826Z","shell.execute_reply.started":"2023-06-23T07:07:20.721336Z","shell.execute_reply":"2023-06-23T07:07:48.295786Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467904/553467096 [==============================] - 23s 0us/step\n553476096/553467096 [==============================] - 23s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocess Image & Extract Features from it","metadata":{}},{"cell_type":"code","source":"# features = {}\n# directory = os.path.join(dataset_dir, 'Images')\n\n# for imgName in tqdm(os.listdir(directory)):\n#     # load the image from file\n#     imgPath = directory + '/' + imgName\n#     image = load_img(imgPath, target_size=(224, 224))\n#     # get image ID (removing file extension)\n#     imageID = imgName.split('.')[0]\n#     # convert image pixels to numpy array\n#     image = img_to_array(image)\n#     # reshape image\n#     image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n#     # preprocess image for vgg model\n#     image = preprocess_input(image)\n#     # extract features\n#     feature = model.predict(image, verbose=0)\n\n#     # store that feature in the images' features list\n#     features[imageID] = feature","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:48.299757Z","iopub.execute_input":"2023-06-23T07:07:48.300202Z","iopub.status.idle":"2023-06-23T07:07:48.306261Z","shell.execute_reply.started":"2023-06-23T07:07:48.300163Z","shell.execute_reply":"2023-06-23T07:07:48.304854Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Store Image Features","metadata":{}},{"cell_type":"code","source":"# store features in pickle\n#pickle.dump(features, open(os.path.join(work_dir, 'features.pkl'), 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:48.307629Z","iopub.execute_input":"2023-06-23T07:07:48.308438Z","iopub.status.idle":"2023-06-23T07:07:48.317911Z","shell.execute_reply.started":"2023-06-23T07:07:48.308397Z","shell.execute_reply":"2023-06-23T07:07:48.316956Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Load Image Features","metadata":{}},{"cell_type":"code","source":"# load features using pickle\nwith open(os.path.join(features_dir, 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:48.319269Z","iopub.execute_input":"2023-06-23T07:07:48.320144Z","iopub.status.idle":"2023-06-23T07:07:49.333282Z","shell.execute_reply.started":"2023-06-23T07:07:48.320089Z","shell.execute_reply":"2023-06-23T07:07:49.332011Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load Captions","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(dataset_dir, 'captions.txt'), 'r') as f:\n    next(f)\n    captions_doc = f.read()","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.336009Z","iopub.execute_input":"2023-06-23T07:07:49.33715Z","iopub.status.idle":"2023-06-23T07:07:49.393304Z","shell.execute_reply.started":"2023-06-23T07:07:49.337081Z","shell.execute_reply":"2023-06-23T07:07:49.392365Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Map Captions to Images","metadata":{}},{"cell_type":"code","source":"# map images' captions to each image\nimgsWithCaptions = {}\n# process lines\nfor line in tqdm(captions_doc.split('\\n')):\n    # split the line by comma(,)\n    tokens = line.split(',')\n    imageName, caption = tokens[0], tokens[1:]\n    # remove extension from image ID\n    imageID = imageName.split('.')[0]\n    # convert caption list to string\n    caption = \" \".join(caption)\n    # create a list for new image\n    if imageID not in imgsWithCaptions:\n        imgsWithCaptions[imageID] = []\n    # store the caption\n    imgsWithCaptions[imageID].append(caption)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.396398Z","iopub.execute_input":"2023-06-23T07:07:49.396719Z","iopub.status.idle":"2023-06-23T07:07:49.549958Z","shell.execute_reply.started":"2023-06-23T07:07:49.396691Z","shell.execute_reply":"2023-06-23T07:07:49.548992Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40456 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387dfdc03c2d4a4a9ab04af49a36e32e"}},"metadata":{}}]},{"cell_type":"code","source":"len(imgsWithCaptions)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.551573Z","iopub.execute_input":"2023-06-23T07:07:49.55229Z","iopub.status.idle":"2023-06-23T07:07:49.561055Z","shell.execute_reply.started":"2023-06-23T07:07:49.552238Z","shell.execute_reply":"2023-06-23T07:07:49.560047Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"8092"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess Caption Data","metadata":{}},{"cell_type":"code","source":"def clean(imgsWithCaptions):\n    for image, captions in imgsWithCaptions.items():\n        for i in range(len(captions)):\n            # take one caption at a time\n            caption = captions[i]\n            # preprocessing steps\n            # convert to lowercase\n            caption = caption.lower()\n            # delete digits, special chars, etc..\n            caption = caption.replace('[^A-Za-z]', '')\n            # delete additional spaces\n            caption = caption.replace('\\s+', ' ')\n            # add start and end tags to the caption\n            caption = 'sseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' eseq'\n            captions[i] = caption","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.567735Z","iopub.execute_input":"2023-06-23T07:07:49.568378Z","iopub.status.idle":"2023-06-23T07:07:49.575919Z","shell.execute_reply.started":"2023-06-23T07:07:49.568341Z","shell.execute_reply":"2023-06-23T07:07:49.575293Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# before preprocess of text\nimgsWithCaptions['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.577435Z","iopub.execute_input":"2023-06-23T07:07:49.578215Z","iopub.status.idle":"2023-06-23T07:07:49.592411Z","shell.execute_reply.started":"2023-06-23T07:07:49.578178Z","shell.execute_reply":"2023-06-23T07:07:49.591415Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['A child in a pink dress is climbing up a set of stairs in an entry way .',\n 'A girl going into a wooden building .',\n 'A little girl climbing into a wooden playhouse .',\n 'A little girl climbing the stairs to her playhouse .',\n 'A little girl in a pink dress going into a wooden cabin .']"},"metadata":{}}]},{"cell_type":"code","source":"# preprocess the text\nclean(imgsWithCaptions)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.593695Z","iopub.execute_input":"2023-06-23T07:07:49.5947Z","iopub.status.idle":"2023-06-23T07:07:49.767359Z","shell.execute_reply.started":"2023-06-23T07:07:49.594663Z","shell.execute_reply":"2023-06-23T07:07:49.766398Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# after preprocess of text\nimgsWithCaptions['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.768901Z","iopub.execute_input":"2023-06-23T07:07:49.769279Z","iopub.status.idle":"2023-06-23T07:07:49.777127Z","shell.execute_reply.started":"2023-06-23T07:07:49.769243Z","shell.execute_reply":"2023-06-23T07:07:49.776088Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['sseq child in pink dress is climbing up set of stairs in an entry way eseq',\n 'sseq girl going into wooden building eseq',\n 'sseq little girl climbing into wooden playhouse eseq',\n 'sseq little girl climbing the stairs to her playhouse eseq',\n 'sseq little girl in pink dress going into wooden cabin eseq']"},"metadata":{}}]},{"cell_type":"code","source":"allCaptions = []\nfor imageID in imgsWithCaptions:\n    for caption in imgsWithCaptions[imageID]:\n        allCaptions.append(caption)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.778546Z","iopub.execute_input":"2023-06-23T07:07:49.779497Z","iopub.status.idle":"2023-06-23T07:07:49.794059Z","shell.execute_reply.started":"2023-06-23T07:07:49.779458Z","shell.execute_reply":"2023-06-23T07:07:49.79318Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"len(allCaptions)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.795856Z","iopub.execute_input":"2023-06-23T07:07:49.796962Z","iopub.status.idle":"2023-06-23T07:07:49.806084Z","shell.execute_reply.started":"2023-06-23T07:07:49.796925Z","shell.execute_reply":"2023-06-23T07:07:49.805065Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"40456"},"metadata":{}}]},{"cell_type":"code","source":"allCaptions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.807411Z","iopub.execute_input":"2023-06-23T07:07:49.808446Z","iopub.status.idle":"2023-06-23T07:07:49.817717Z","shell.execute_reply.started":"2023-06-23T07:07:49.808411Z","shell.execute_reply":"2023-06-23T07:07:49.816497Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['sseq child in pink dress is climbing up set of stairs in an entry way eseq',\n 'sseq girl going into wooden building eseq',\n 'sseq little girl climbing into wooden playhouse eseq',\n 'sseq little girl climbing the stairs to her playhouse eseq',\n 'sseq little girl in pink dress going into wooden cabin eseq',\n 'sseq black dog and spotted dog are fighting eseq',\n 'sseq black dog and tri-colored dog playing with each other on the road eseq',\n 'sseq black dog and white dog with brown spots are staring at each other in the street eseq',\n 'sseq two dogs of different breeds looking at each other on the road eseq',\n 'sseq two dogs on pavement moving toward each other eseq']"},"metadata":{}}]},{"cell_type":"code","source":"# tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(allCaptions)\nuniqueWordsCount = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:49.8195Z","iopub.execute_input":"2023-06-23T07:07:49.819983Z","iopub.status.idle":"2023-06-23T07:07:50.543383Z","shell.execute_reply.started":"2023-06-23T07:07:49.819948Z","shell.execute_reply":"2023-06-23T07:07:50.5424Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Save the tokenizer\nwith open('captions_tokenizer.pickle', 'wb') as t:\n    pickle.dump(tokenizer, t)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.544712Z","iopub.execute_input":"2023-06-23T07:07:50.545818Z","iopub.status.idle":"2023-06-23T07:07:50.560462Z","shell.execute_reply.started":"2023-06-23T07:07:50.545777Z","shell.execute_reply":"2023-06-23T07:07:50.559577Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer\nwith open('captions_tokenizer.pickle', 'rb') as t:\n    tokenizer = pickle.load(t)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.56176Z","iopub.execute_input":"2023-06-23T07:07:50.562217Z","iopub.status.idle":"2023-06-23T07:07:50.577751Z","shell.execute_reply.started":"2023-06-23T07:07:50.562179Z","shell.execute_reply":"2023-06-23T07:07:50.576896Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"uniqueWordsCount","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.578991Z","iopub.execute_input":"2023-06-23T07:07:50.57944Z","iopub.status.idle":"2023-06-23T07:07:50.586426Z","shell.execute_reply.started":"2023-06-23T07:07:50.579404Z","shell.execute_reply":"2023-06-23T07:07:50.585159Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"8485"},"metadata":{}}]},{"cell_type":"code","source":"# get maximum length of the caption available\nmaxCaptionLength = max(len(caption.split()) for caption in allCaptions)\nmaxCaptionLength","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.588866Z","iopub.execute_input":"2023-06-23T07:07:50.589972Z","iopub.status.idle":"2023-06-23T07:07:50.629024Z","shell.execute_reply.started":"2023-06-23T07:07:50.589937Z","shell.execute_reply":"2023-06-23T07:07:50.628037Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"35"},"metadata":{}}]},{"cell_type":"markdown","source":"# Split (Train-Test) Images","metadata":{}},{"cell_type":"code","source":"imageIDs = list(imgsWithCaptions.keys())\nsplitPercentage = int(len(imageIDs) * 0.90)\ntrainImgs = imageIDs[:splitPercentage]\ntestImgs = imageIDs[splitPercentage:]\n\n#print(splitPercentage)\nprint(\"train images: \",len(trainImgs),\"\\ntest  images: \",len(testImgs))\n#len(train + test)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.630199Z","iopub.execute_input":"2023-06-23T07:07:50.63052Z","iopub.status.idle":"2023-06-23T07:07:50.637641Z","shell.execute_reply.started":"2023-06-23T07:07:50.630486Z","shell.execute_reply":"2023-06-23T07:07:50.636517Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"train images:  7282 \ntest  images:  810\n","output_type":"stream"}]},{"cell_type":"code","source":"#        '<s> girl going into wooden building <e>'\n\n#        (capt_VectorSeq)                             (newSeq)\n\n#         <s>                                          girl\n#         <s> girl                                     going\n#         <s> girl going                               into\n#         <s> girl going into                          wooden\n#         <s> girl going into wooden                   building    \n#         <s> girl going into wooden building          <e>","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.639006Z","iopub.execute_input":"2023-06-23T07:07:50.640092Z","iopub.status.idle":"2023-06-23T07:07:50.648146Z","shell.execute_reply.started":"2023-06-23T07:07:50.640056Z","shell.execute_reply":"2023-06-23T07:07:50.647251Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# function to train the model for predicting the caption sequence\ndef sequenceGenerator(imageIDs, imgsWithCaptions, features, tokenizer, maxCaptionLength, uniqueWordsCount, batchSize):\n    # loop over images\n    img_FeatureSeq, capt_VectorSeq, newSeq = list(), list(), list()\n    n = 0\n    while 1:\n        for imageID in imageIDs:\n            n += 1\n            captions = imgsWithCaptions[imageID]\n            # process each caption\n            for caption in captions:\n                # encode the sequence\n                captionSeq = tokenizer.texts_to_sequences([caption])[0]\n                # split the sequence into X, y pairs\n                for i in range(1, len(captionSeq)):\n                    # split into input and output pairs\n                    seqIn, seqOut = captionSeq[:i], captionSeq[i]\n                    # pad input sequence\n                    seqIn = pad_sequences([seqIn], maxlen=maxCaptionLength)[0]\n                    # encode output sequence\n                    seqOut = to_categorical([seqOut], num_classes=uniqueWordsCount)[0]\n                    \n                    # store the sequences\n                    img_FeatureSeq.append(features[imageID][0])\n                    capt_VectorSeq.append(seqIn)\n                    newSeq.append(seqOut)\n                    \n            if n == batchSize:\n                img_FeatureSeq, capt_VectorSeq, newSeq = np.array(img_FeatureSeq), np.array(capt_VectorSeq), np.array(newSeq)\n                yield [img_FeatureSeq, capt_VectorSeq], newSeq\n                img_FeatureSeq, capt_VectorSeq, newSeq = list(), list(), list()\n                n = 0","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.649626Z","iopub.execute_input":"2023-06-23T07:07:50.65014Z","iopub.status.idle":"2023-06-23T07:07:50.663744Z","shell.execute_reply.started":"2023-06-23T07:07:50.650079Z","shell.execute_reply":"2023-06-23T07:07:50.662596Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Reshaping our Model","metadata":{}},{"cell_type":"code","source":"# IMAGE LAYERS\nimagesInput = Input(shape=(4096,))\nimagesDrop = Dropout(0.6)(imagesInput)\nimagesOutput = Dense(256, activation='relu')(imagesDrop)\n\n# CAPTION LAYERS\ncaptionsInput = Input(shape=(maxCaptionLength,))\ncaptionsEmbed = Embedding(uniqueWordsCount, 256, mask_zero=True)(captionsInput)\ncaptionsDrop = Dropout(0.6)(captionsEmbed)\ncaptionsOutput = LSTM(256)(captionsDrop)\n\n# MERGE IMAGE AND CAPTION LAYERS\nmergedLayers = add([imagesOutput, captionsOutput])\nmergeOutput = Dense(256, activation='relu')(mergedLayers)\nOutput = Dense(uniqueWordsCount, activation='softmax')(mergeOutput)\n\n# OUR IMAGE CAPTION GENERATOR (icg) MODEL\nicg_model = Model(inputs=[imagesInput, captionsInput], outputs = Output)\nicg_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:50.665213Z","iopub.execute_input":"2023-06-23T07:07:50.665653Z","iopub.status.idle":"2023-06-23T07:07:51.549345Z","shell.execute_reply.started":"2023-06-23T07:07:50.665614Z","shell.execute_reply":"2023-06-23T07:07:51.548227Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"epochs = 35\nbatchSize = 20\nsteps = len(trainImgs) // batchSize\n\nfor i in range(epochs):\n    # create the output sequence\n    generator = sequenceGenerator(imageIDs, imgsWithCaptions, features, tokenizer, maxCaptionLength, uniqueWordsCount, batchSize)\n    # train model on the output sequence\n    icg_model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:07:51.550769Z","iopub.execute_input":"2023-06-23T07:07:51.551218Z","iopub.status.idle":"2023-06-23T07:25:35.285402Z","shell.execute_reply.started":"2023-06-23T07:07:51.551176Z","shell.execute_reply":"2023-06-23T07:25:35.282948Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"364/364 [==============================] - 81s 210ms/step - loss: 5.0555 - accuracy: 0.1685\n364/364 [==============================] - 76s 209ms/step - loss: 3.9544 - accuracy: 0.2576\n364/364 [==============================] - 79s 216ms/step - loss: 3.5685 - accuracy: 0.2877\n364/364 [==============================] - 76s 208ms/step - loss: 3.3168 - accuracy: 0.3064\n364/364 [==============================] - 76s 209ms/step - loss: 3.1297 - accuracy: 0.3215\n364/364 [==============================] - 79s 217ms/step - loss: 2.9854 - accuracy: 0.3342\n364/364 [==============================] - 78s 213ms/step - loss: 2.8701 - accuracy: 0.3464\n364/364 [==============================] - 75s 207ms/step - loss: 2.7810 - accuracy: 0.3565\n364/364 [==============================] - 76s 207ms/step - loss: 2.7084 - accuracy: 0.3653\n364/364 [==============================] - 78s 214ms/step - loss: 2.6432 - accuracy: 0.3736\n364/364 [==============================] - 77s 212ms/step - loss: 2.5899 - accuracy: 0.3802\n364/364 [==============================] - 75s 207ms/step - loss: 2.5411 - accuracy: 0.3865\n364/364 [==============================] - 77s 212ms/step - loss: 2.4961 - accuracy: 0.3926\n124/364 [=========>....................] - ETA: 52s - loss: 2.4505 - accuracy: 0.3988","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/1913548507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequenceGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageIDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsWithCaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCaptionLength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniqueWordsCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# train model on the output sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0micg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Save Trained Model","metadata":{}},{"cell_type":"code","source":"icg_model.save(work_dir + '/icgModel_6.h5')","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:25:35.286812Z","iopub.status.idle":"2023-06-23T07:25:35.287606Z","shell.execute_reply.started":"2023-06-23T07:25:35.287319Z","shell.execute_reply":"2023-06-23T07:25:35.287345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Trained Model","metadata":{}},{"cell_type":"code","source":"#icg_model = tf_models.load_model('/kaggle/input/icgmodel/icgModel.h5')\nplot_model(icg_model, show_shapes=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:25:35.289021Z","iopub.status.idle":"2023-06-23T07:25:35.289777Z","shell.execute_reply.started":"2023-06-23T07:25:35.289513Z","shell.execute_reply":"2023-06-23T07:25:35.289538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def indexToWord(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:25:35.291155Z","iopub.status.idle":"2023-06-23T07:25:35.2919Z","shell.execute_reply.started":"2023-06-23T07:25:35.291632Z","shell.execute_reply":"2023-06-23T07:25:35.291657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict caption sequence for an image\ndef imgPredictCaption(model, imageFeatures, tokenizer, maxCaptionLength):\n    # add start tag for generation process\n    caption = 'sseq'\n    # iterate over the max length of sequence\n    for i in range(maxCaptionLength):\n        # encode input sequence\n        sequence = tokenizer.texts_to_sequences([caption])[0]\n        # pad the sequence\n        sequence = pad_sequences([sequence], maxCaptionLength)\n        # predict next word\n        predictions = model.predict([imageFeatures, sequence], verbose=0)\n        # get word index with the highest probability\n        outputWordIndex = np.argmax(predictions)\n        # convert index to word\n        word = indexToWord(outputWordIndex, tokenizer)\n        # stop if word is not found in vocabulary\n        if word is None:\n            break\n        # append word as input for generating next word\n        caption += \" \" + word\n        # stop if we reach the end of sequence\n        if word == 'eseq':\n            break\n      \n    return caption","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:25:35.293277Z","iopub.status.idle":"2023-06-23T07:25:35.294027Z","shell.execute_reply.started":"2023-06-23T07:25:35.293758Z","shell.execute_reply":"2023-06-23T07:25:35.293783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prototype","metadata":{}},{"cell_type":"code","source":"# load the image #1003163366_44323f5815\ntestImg_path = '/kaggle/input/flickr8k/Images/1003163366_44323f5815.jpg'\n\ndef generateCaption(img_path):\n    image = load_img(img_path, target_size=(224, 224))\n    # convert image pixels to numpy array\n    image = img_to_array(image)\n    # reshape image\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # preprocess image for vgg\n    image = preprocess_input(image)\n    # extract features\n    image_features = model.predict(image, verbose=0)\n\n    generated_caption = imgPredictCaption(icg_model, image_features, tokenizer, maxCaptionLength)\n    plt.imshow(Image.open(img_path))\n    actualCaptions = imgsWithCaptions['1003163366_44323f5815']\n\n    print(\"\\n      Actual Captions\\n\")\n    for caption in actualCaptions:\n        print(caption)\n\n    print(\"\\n      Generated Caption\\n\")  \n    print(generated_caption, \"\\n\")\n    \n    \ngenerateCaption(testImg_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:25:35.295399Z","iopub.status.idle":"2023-06-23T07:25:35.296158Z","shell.execute_reply.started":"2023-06-23T07:25:35.295874Z","shell.execute_reply":"2023-06-23T07:25:35.295898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Model Accuracy in BLEU Score","metadata":{}},{"cell_type":"code","source":"actualCaptions = list()\npredictedCaptions = list()\n\nn = 0\nfor imageID in tqdm(testImgs):\n    if n == 809:\n        break\n    # read the 5 actual captions of the image\n    imgActualCaptions = imgsWithCaptions[imageID]\n    \n    #if(imageID == '539667015_fd0a3bea07'):\n        #print(imgActualCaptions)\n        #print('\\n')\n        \n    # split each of the 5 captions into words (array of words with each caption)\n    imgActualCaptions = [caption.split() for caption in imgActualCaptions]\n    # append these actual captions to the total actual captions list\n    actualCaptions.append(imgActualCaptions)\n    \n    # get the predicted caption of the image\n    imgPredictedCaption = imgPredictCaption(icg_model, features[imageID], tokenizer, maxCaptionLength)   \n    imgPredictedCaption = imgPredictedCaption.split()\n    \n    # append these predicted caption to the total predicted captions list\n    predictedCaptions.append(imgPredictedCaption)\n    n += 1","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:25:35.297517Z","iopub.status.idle":"2023-06-23T07:25:35.298259Z","shell.execute_reply.started":"2023-06-23T07:25:35.297978Z","shell.execute_reply":"2023-06-23T07:25:35.298002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\nprint(\"BLEU-1: %f\" %corpus_bleu(actualCaptions, predictedCaptions, weights=(1.0, 0, 0, 0)))\nprint(\"BLEU-2: %f\" %corpus_bleu(actualCaptions, predictedCaptions, weights=(0.5, 0.5, 0, 0)))","metadata":{"execution":{"iopub.status.busy":"2023-06-23T07:25:35.299592Z","iopub.status.idle":"2023-06-23T07:25:35.300326Z","shell.execute_reply.started":"2023-06-23T07:25:35.300049Z","shell.execute_reply":"2023-06-23T07:25:35.300073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}